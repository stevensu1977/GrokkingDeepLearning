## 学习、梯度下降、反向传播(back propagation)

梯度下降、反向传播这部分花了我很多时间去理解，阅读，本章主要是记录我个人的理解体会，没有使用复杂的数据推导。

1. 预测、比较和学习

   在上一章我学习到了一个最简单的神经网络, 输入，权重，预测。

   > 为让神经网络能够准备进行预测，我们应该如何设置权重?  p42

   有了预测，我们需要把预测和真实结果进行比较, 本书使用了一种简单的测量误差的方法: "均方误差"

   ```python
   lable = 0.8 #真实值
   weight = 0.5 #权重
   input = 2 #输入
   
   def neural_network(input, weight):
       return input * weight
   prediction = neural_network(input,weight) # 预测为 1
   
   error=(prediction - lable)**2 #误差0.04
   
   ```

   有了比较只是让我们知道了模型错了多少,我们还需要让它学习.

   > "学习" 告诉权重应该如何改变以降低误差 p42

    

2. 冷热学习

   冷热学习法，通过扰动(try)权重来确定哪个方向可以使得误差的降低幅度最大， 简单来说就是做出预测后，在按照固定步长和权重做一次加法，一次减法，再观察预测误差，那个误差小选那个进行权重更新。冷热法是最简单的方法，问题是1次迭代它就需要做额外的2次(+, - )，总共3次预测, 整体效率低下，我们知道调节权重的方向，但是我们也只能使用一个固定的数值作为步长,这个和误差并无直接关系.

   

3. 梯度下降, 核心关键词: 纯误差、缩放、负值反转和停止(待更新)

   

    

    

   

