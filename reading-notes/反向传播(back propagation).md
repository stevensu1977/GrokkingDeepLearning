## 学习、梯度下降、反向传播(back propagation)

梯度下降、反向传播这部分花了我很多时间去理解，阅读，本章主要是记录我个人的理解体会，没有使用复杂的数据推导,为了简单我并没有启用bias(b),。

1. 预测、比较和学习

   在上一章我学习到了一个最简单的神经网络, 输入，权重，预测。

   > 为让神经网络能够准备进行预测，我们应该如何设置权重?  p42

   有了预测，我们需要把预测和真实结果进行比较, 本书使用了一种简单的测量误差的方法: "均方误差"

   ```python
   lable = 0.8 #真实值
   weight = 0.5 #权重
   input = 2 #输入
   
   def neural_network(input, weight):
       return input * weight
   prediction = neural_network(input,weight) # 预测为 1
   
   error=(prediction - lable)**2 #误差0.04
   
   ```

   有了比较只是让我们知道了模型错了多少,我们还需要让它学习.

   > "学习" 告诉权重应该如何改变以降低误差 p42

    

2. 冷热学习

   冷热学习法，通过扰动(try)权重来确定哪个方向可以使得误差的降低幅度最大， 简单来说就是做出预测后，在按照固定步长和权重做一次加法，一次减法，再观察预测误差，那个误差小选那个进行权重更新。冷热法是最简单的方法，问题是1次迭代它就需要做额外的2次(+, - )，总共3次预测, 整体效率低下，我们知道调节权重的方向，但是我们也只能使用一个固定的数值作为步长,这个和误差并无直接关系.

   

3. 梯度下降, 核心关键词: 缩放、负值反转和停止

   在冷热学习法之后我们就要学习到通用的调整weight的做法了:梯度下降, 先排除数学公式，我们可以假想自己是一个登山者已经站在了山顶上，我要下山到达营地(最小误差)，肯定是希望找到一个最优路径，问题是如果我没有地图、导航，那怎么办？我可能就会尝试一小段一小段的路来走，方向，距离(大小、长短)尤其重要，这个就可以粗略的理解为梯度下降。

   

   > 数学推导:
   >
   > ​        导数是一元函数的变化率（斜率），如果是多元函数则称之为偏导数。偏导数是多元函数“退化”成一元函数时的导数，这里“退化”的意思是固定其他变量的值，只保留一个变量，依次保留每个变量，则𝑁元函数有𝑁个偏导数。偏导数为坐标轴方向上的方向导数，其他方向的方向导数为偏导数的合成。写成向量形式，偏导数构成的向量为∇𝑓(𝑎,𝑏)=(𝑓𝑥(𝑎,𝑏),𝑓𝑦(𝑎,𝑏))，称之为梯度。

   

   缩放,  f(x) =x*w + b ,  那么w和什么有关，从导数的角度来说，w只和x也就是这个input有关，input大影响也大。

   负值反转，输入x的正负很重要，决定了权重误差的方向，增大还是减小。

   停止如果输入为0 ,那么x*w也会为0， 就不需要调整权重了。

   ```python
   #单层weight_delta 权重误差计算
   delta = prediction - lable
   weight_delta = alpha * (delta*input) 
   weight = weight - weight_delta
   
   ```

   单层情况下看起来比较简单,但是我们使用了网络堆叠的时候问题就会变得有些复杂,让我们看看增加一个隐藏层的计算

   ```python
   #多层计算
   weight_0_1 = ....
   weight_1_2 = ....
   
   layer_0 = image[0:1]
   layer_1 = relu(layer_0 * weight_0_1)
   layer_2 = layger_1 * weight_1_2
   
   #如何求每一层的delta是反向传播的关键
   layer_2_delta = layer_2 - label[0:1]
   layer_1_delta = layer_2_delta.T.dot(weight_1_2)*relu2deriv(layer_1) 
   
   #更新每一层权重
   weight_1_2= weight_1_2 - alpha *(layer_1.T.dot(layer_2_delta) ) 
   weight_0_1= weight_0_1 - alpha *(layer_0.T.dot(layer_1_delta) )
   
    
   ```

   通过上述的计算就可以自动更新每层的权重, 达到学习的目录.

   

4. 遗留问题: 完整的数学推导

   

   



 

 



